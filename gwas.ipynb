{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysam\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython import embed\n",
    "\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genome_helpers import (\n",
    "#     # compute_af,\n",
    "#     inverse_rank_normalization,\n",
    "    read_annotation_data,\n",
    "    process_plant_phenotype,\n",
    "    get_genome_metadata\n",
    ")\n",
    "\n",
    "from gwas_helpers import (\n",
    "    adj_phenotypes_for_gwas,\n",
    "    annotate_results,\n",
    "    run_gwas,\n",
    "    qqplot,\n",
    "    manhattan_static,\n",
    "    manhattan_interactive\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_af(record):\n",
    "    dp4 = record.info['DP4']\n",
    "    if sum(dp4) == 0:\n",
    "        return None\n",
    "    allele_freq = (dp4[2]+dp4[3]) / sum(dp4)\n",
    "    return allele_freq\n",
    "\n",
    "\n",
    "def get_dp4(record):\n",
    "    \n",
    "    dp4 = record.info['DP4']\n",
    "    if sum(dp4) == 0:\n",
    "        return None\n",
    "    \n",
    "    return dp4\n",
    "\n",
    "\n",
    "def get_freqs_from_vcf(vcf_file):\n",
    "    \n",
    "    bgi_id = os.path.basename(vcf_file).replace(\".vcf.gz\", \"\")\n",
    "\n",
    "    try:\n",
    "        vcf_file = pysam.VariantFile(vcf_file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {bgi_id: None}, {bgi_id: None}\n",
    "        \n",
    "    variant_freq   = {(record.chrom, record.pos): compute_af(record) for record in vcf_file.fetch()}\n",
    "    variant_counts = {(record.chrom, record.pos): get_dp4(record)    for record in vcf_file.fetch()}\n",
    "\n",
    "    # variant_dict.update({sample_info: set(variant_positions)})\n",
    "    # variant_freq_dict.update({bgi_id: variant_freq})\n",
    "\n",
    "    return {bgi_id: variant_freq}, {bgi_id: variant_counts}\n",
    "\n",
    "\n",
    "def get_sample_info(sample_id, metadata):\n",
    "\n",
    "    sample_info = metadata.loc[metadata.BGI_ID == sample_id, [\"generation\", \"rep\", \"treatment\"]]    \n",
    "\n",
    "    if len(sample_info) == 1:\n",
    "        sample_info = sample_info.iloc[0].to_list()\n",
    "        sample_info = tuple(sample_info)\n",
    "    elif len(sample_info) == 0:\n",
    "        print(ValueError(f\"Sample with {sample_id} has no corresponding metadata.\")) \n",
    "        return None\n",
    "    else:\n",
    "        raise(ValueError(f\"Sample with {sample_id} has more than one associated sample.\"))\n",
    "\n",
    "    return sample_info\n",
    "\n",
    "\n",
    "def process_vcf_folder(vcf_folder, cache_file=\"vcf_dictionary.pkl\", as_dataframe=False):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    files = [ file for file in os.listdir(vcf_folder) if file.endswith(\".vcf.gz\") or file.endswith(\".vcf\") ]\n",
    "\n",
    "    variant_freq_dicts = {}\n",
    "    variant_count_dicts = {}\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "           \n",
    "        sample_id = file.replace(\".vcf.gz\", \"\")\n",
    "        # sample_info = get_sample_info(sample_id, genome_metadata)        \n",
    "        variant_freq_dict, variant_count_dict = get_freqs_from_vcf(f\"{vcf_folder}/{file}\")\n",
    "\n",
    "        variant_freq_dicts.update(variant_freq_dict)\n",
    "        variant_count_dicts.update(variant_count_dict)\n",
    "                \n",
    "    if as_dataframe:\n",
    "        return pd.DataFrame(variant_freq_dicts), pd.DataFrame(variant_count_dicts)\n",
    "    else:\n",
    "        return variant_freq_dicts, variant_count_dicts\n",
    "    \n",
    "\n",
    "def filter_variants(freq_df, freq_threshold=0.98, non_missing=450):\n",
    "    \n",
    "    freq_df = freq_df[ freq_df.apply(lambda row: ~(row.dropna() > freq_threshold).all(), axis=1) ]\n",
    "    freq_df = freq_df[ freq_df.apply(lambda row: row.isna().sum(), axis=1) < non_missing ]\n",
    "    \n",
    "    return freq_df\n",
    "\n",
    "\n",
    "def display_df(df, n=5, disable=False, text=\"\", info=True):\n",
    "    \n",
    "    if not disable:        \n",
    "        if text: print(text)\n",
    "        display(df.sample(n))\n",
    "        print(f\"{df.shape=}\")\n",
    "        if info: print(df.info())\n",
    "        print(\"-\"*100)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_bases(row):\n",
    "    \n",
    "    base_calls = row[\"bases\"]\n",
    "    quality_scores = phred_quality(row[\"qual\"])\n",
    "    ref_base = row[\"ref\"].upper()\n",
    "\n",
    "    base_calls = base_calls.replace(\"$\", \"\").replace(\"^]\", \"\").replace(\"^I\", \"\")\n",
    "    \n",
    "    processed_bases = []\n",
    "    passing_quality_scores = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < row[\"depth\"]:\n",
    "        char = base_calls[i]\n",
    "\n",
    "        # Filtrar por calidad (descartar bases con calidad < 20)\n",
    "        if quality_scores[i] < 20:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        passing_quality_scores.append(quality_scores[i])\n",
    "        \n",
    "        # Reemplazar referencia\n",
    "        if char in \".,\":  \n",
    "            processed_bases.append(ref_base)\n",
    "\n",
    "        # Contar bases normales\n",
    "        elif char.upper() in \"ACTG\":\n",
    "            processed_bases.append(char.upper())\n",
    "\n",
    "        # Contar deleciones en la referencia (`*`)\n",
    "        elif char == \"*\":\n",
    "            processed_bases.append(\"D\")\n",
    "\n",
    "        # Detectar inserciones (`+nX`)\n",
    "        elif char == \"+\":\n",
    "            match = re.match(r\"\\+(\\d+)\", base_calls[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                inserted_seq = base_calls[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "                # processed_bases.append(f\"INS_{inserted_seq.upper()}\")\n",
    "                processed_bases.append(\"I\")\n",
    "                i += len(match.group(1)) + num_bases\n",
    "\n",
    "        # Detectar deleciones (`-nX`)\n",
    "        elif char == \"-\":\n",
    "            match = re.match(r\"\\-(\\d+)\", base_calls[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                deleted_seq = base_calls[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "                processed_bases.append(\"D\")\n",
    "                # processed_bases.append(f\"DEL_{deleted_seq.upper()}\")\n",
    "                i += len(match.group(1)) + num_bases\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if len(passing_quality_scores) == 0:\n",
    "         allele_counts = { 'A': 0, 'C': 0, 'T': 0, 'G': 0, 'DEL': 0, 'INS': 0, \"Avg_Qual\": None, \"depth\": 0, \"depth_high_q\": 0 }\n",
    "         return allele_counts\n",
    "\n",
    "    allele_counts = {\n",
    "        'A': processed_bases.count('A'),\n",
    "        'C': processed_bases.count('C'),\n",
    "        'T': processed_bases.count('T'),\n",
    "        'G': processed_bases.count('G'),\n",
    "        'DEL': processed_bases.count('D'),\n",
    "        'INS': processed_bases.count('I')\n",
    "    }\n",
    "\n",
    "    # Contar inserciones y deleciones específicas\n",
    "    for item in processed_bases:\n",
    "        if item.startswith(\"INS_\"):\n",
    "            allele_counts[item] = allele_counts.get(item, 0) + 1\n",
    "        if item.startswith(\"DEL_\"):\n",
    "            allele_counts[item] = allele_counts.get(item, 0) + 1\n",
    "\n",
    "    # Calcular calidad promedio\n",
    "    # allele_counts[\"Avg_Qual\"] = np.mean(quality_scores) if quality_scores else 0\n",
    "    allele_counts[\"Avg_Qual\"] = sum(passing_quality_scores) / len(passing_quality_scores) # if quality_scores else 0\n",
    "    allele_counts[\"depth\"] = row[\"depth\"]\n",
    "    allele_counts[\"depth_high_q\"] = len(passing_quality_scores)\n",
    "    allele_counts[\"bases\"] = processed_bases\n",
    "    allele_counts[\"original_bases\"] = row[\"bases\"]\n",
    "    allele_counts[\"ref_base\"] = ref_base\n",
    "    allele_counts[\"quality_scores\"] = quality_scores\n",
    "    \n",
    "    return allele_counts\n",
    "\n",
    "\n",
    "def mismatch(row):\n",
    "    alleles = [ row[x] for x in ['A', 'C', 'T', 'G', 'DEL', 'INS'] ] #, row.C, row.T, row.G]#, row.DEL, row.INS]\n",
    "    # print(sum(alleles) != row.depth_high_q)\n",
    "    return sum(alleles) != row.depth_high_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Para obtener la frecuencia de los alelos alternativo, usamos el campo `DP4` del archivo VCF, con las siguientes componentes:\n",
    "\n",
    "| Campo   | Descripción                                      |\n",
    "|---------|------------------------------------------------|\n",
    "| DP4[0]  | Reads forward para el alelo de referencia.    |\n",
    "| DP4[1]  | Reads reverse para el alelo de referencia.    |\n",
    "| DP4[2]  | Reads forward para el alelo alternativo.      |\n",
    "| DP4[3]  | Reads reverse para el alelo alternativo.      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "- **Input**: matriz con cuentas para cada alelo, donde `NaN` significa que el SNV no tiene _variaciones o cobertura_.\n",
    "- Ver cobertura para SNVs que no aparecen en los VCFs.\n",
    "- Filtrar variantes.\n",
    "- filas:(planta,SNV),columnas:None -> filas:planta,columnas:SNV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = False\n",
    "VCF_DIR = \"data/genomes/alignments_paired_end_new/\"\n",
    "CACHED_FREQ_PKL = \"freq_dataframe_ref2.pkl\"\n",
    "CACHED_FREQ_WIDE_PKL = \"freq_dataframe_wide_ref2.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert not os.path.exists(CACHED_FREQ_PKL), f\"Skipping this cell as {CACHED_FREQ_PKL} already exists.\"\n",
    "\n",
    "batch_mapping = get_genome_metadata(as_dataframe=False)\n",
    "freq_df, counts_df = process_vcf_folder(vcf_folder=VCF_DIR, as_dataframe=True)\n",
    "depth_df  = counts_df.map(lambda x: sum(x) if isinstance(x, tuple) else x)\n",
    "\n",
    "freq_df = ( freq_df\n",
    "    .pipe(display_df, text=\"Pre-variant filtering\", disable=not VERBOSE)\n",
    "    .pipe(filter_variants)\n",
    "    .pipe(display_df, text=\"Filtered variants\", disable=not VERBOSE)\n",
    "    .melt(ignore_index=False)\n",
    "    .pipe(display_df, text=\"Reshaped\", disable=not VERBOSE)\n",
    "    .reset_index()\n",
    "    .pipe(display_df, text=\"Reset index\", disable=not VERBOSE)\n",
    "    .rename({\"level_0\": \"contig\", \"level_1\": \"position\", \"value\": \"freq\"}, axis=1)\n",
    "    .pipe(display_df, text=\"Renamed columns\", disable=not VERBOSE)\n",
    ")\n",
    "\n",
    "# We map the batch to the sample so that we can pair it up with the phenotype values from the other file\n",
    "samples = freq_df.variable.apply(lambda x: batch_mapping.get(x, (None, None, -1)))\n",
    "samples_df = pd.DataFrame(samples.to_list(), columns=[\"treatment\", \"replica\", \"generation\"])\n",
    "\n",
    "freq_df = pd.concat([ freq_df, samples_df ], axis=1)\n",
    "freq_df = freq_df.query(\"generation != -1\")\n",
    "freq_df.replica = freq_df.replica.astype(int)\n",
    "freq_df = freq_df[~freq_df.freq.isna()]\n",
    "\n",
    "pkl.dump(freq_df, open(CACHED_FREQ_PKL, \"wb\"))\n",
    "\n",
    "# assert not os.path.exists(CACHED_FREQ_PKL), f\"Skipping this cell as {CACHED_FREQ_PKL} already exists.\"\n",
    "\n",
    "def merge_contig_and_position(df): \n",
    "    return df.assign(variant_id=df[[\"contig\", \"position\"]].apply(tuple, axis=1))\n",
    "\n",
    "\n",
    "freq_df = ( \n",
    "    pkl.load(open(CACHED_FREQ_PKL, \"rb\"))\n",
    "    .merge_contig_and_position\n",
    "    .drop([\"contig\", \"position\"], axis=1)\n",
    "    .pivot(\n",
    "        columns=['treatment', 'replica', 'generation'], \n",
    "        index=['variant_id'], \n",
    "        values='freq'\n",
    "    )\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "pkl.dump(freq_df, open(CACHED_FREQ_WIDE_PKL, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_contig_and_position(df): \n",
    "    return df.assign(variant_id=df[[\"contig\", \"position\"]].apply(tuple, axis=1))\n",
    "\n",
    "\n",
    "freq_df = ( \n",
    "    pkl.load(open(CACHED_FREQ_PKL, \"rb\"))\n",
    "    .pipe(merge_contig_and_position)\n",
    "    .drop([\"contig\", \"position\"], axis=1)\n",
    "    .pivot(\n",
    "        columns=['treatment', 'replica', 'generation'], \n",
    "        index=['variant_id'], \n",
    "        values='freq'\n",
    "    )\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "pkl.dump(freq_df, open(CACHED_FREQ_WIDE_PKL, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_generations_with_many_nans(freq_df, min_n_of_nan):\n",
    "    return freq_df.loc[ freq_df.isna().__invert__().sum(axis=1) > min_n_of_nan ]\n",
    "\n",
    "\n",
    "def extract_top_n_variable_variants(freq_df, n):\n",
    "    top_changing_variants = freq_df.std(axis=1).sort_values(ascending=False).iloc[:n]\n",
    "    freq_df = freq_df.loc[top_changing_variants.index]\n",
    "    return freq_df\n",
    "\n",
    "\n",
    "def join_treatment_rep_gen(freq_df):    \n",
    "    return freq_df.set_index(freq_df.index.to_frame().apply(tuple, axis=1).rename(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NUMBER_OF_NON_NAN = 200\n",
    "TOP_N_VARIANTS = 200\n",
    "\n",
    "freq_df = (\n",
    "    pkl.load(open(CACHED_FREQ_WIDE_PKL, \"rb\"))\n",
    "    .astype(float)\n",
    "    .pipe(eliminate_generations_with_many_nans, MIN_NUMBER_OF_NON_NAN)\n",
    "    .pipe(extract_top_n_variable_variants, TOP_N_VARIANTS)\n",
    "    .transpose()\n",
    "    .pipe(join_treatment_rep_gen)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS = [(1, 'K'), (1, 'MS'), (2, 'MS'), (2, 'K'), (3, 'K'), (3, 'MS')]\n",
    "PHENOTYPES = [\"PR_Length\", \"LR_number\", \"LR_Density\"]\n",
    "\n",
    "phenotypes_df, _, _, phenotypes_df_red_nb = process_plant_phenotype()\n",
    "phenotypes_df = adj_phenotypes_for_gwas(phenotypes_df, phenotypes_df_red_nb, PHENOTYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### GWAS: one plant, one data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_results.set_index([\"SNP\", \"phenotype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNV = (\"PRKV01000002.1\", 42521)\n",
    "# SNV = (\"PRKV01000002.1\", 496961)\n",
    "SNV = (\"PRKV01000009.1\", 124951)\n",
    "\n",
    "shuffled_results = []\n",
    "\n",
    "for i in range(1000):\n",
    "\n",
    "    snp_freq_df = freq_df[[SNV]]\n",
    "    shuffled_values = snp_freq_df.values\n",
    "    np.random.shuffle(shuffled_values)\n",
    "    freq_shuffled_df = pd.DataFrame(shuffled_values.reshape(snp_freq_df.shape), index=snp_freq_df.index, columns=snp_freq_df.columns)\n",
    "    \n",
    "    all_shuffled_data = pd.merge(phenotypes_df, freq_shuffled_df, left_index=True, right_index=True)\n",
    "    snps = freq_shuffled_df.columns.to_list()\n",
    "    gwas_shuffled_results = run_gwas(all_shuffled_data, snps, phenotypes=PHENOTYPES)\n",
    "    shuffled_results.append(gwas_shuffled_results)\n",
    "\n",
    "shuffled_results = pd.concat(shuffled_results)\n",
    "\n",
    "# shuffled_results.query(\"phenotype == 'PR_Length'\").p_value.hist(bins=100);\n",
    "# shuffled_results.query(\"phenotype == 'PR_Length'\").p_value.apply(np.log10).hist(bins=50);\n",
    "shuffled_results.query(\"phenotype == 'LR_number'\").p_value.apply(lambda x: -np.log10(x)).hist(bins=50);\n",
    "# shuffled_results.query(\"phenotype == 'LR_Density'\").p_value.apply(np.log10).hist(bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(phenotypes_df, freq_df, left_index=True, right_index=True)\n",
    "snps = freq_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_results = run_gwas(all_data, snps, phenotypes=PHENOTYPES)\n",
    "# gwas_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(gwas_results.query(\"phenotype == 'PR_Length'\").p_value)\n",
    "qqplot(gwas_results.query(\"phenotype == 'LR_number'\").p_value)\n",
    "qqplot(gwas_results.query(\"phenotype == 'LR_Density'\").p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "snps = gwas_results.SNP.iloc[3]\n",
    "all_data[snps].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(df, num_permutations=1000):\n",
    "    \"\"\"\n",
    "    Realiza un test de permutación restringida en el GWAS.\n",
    "    - Permuta phenotype_diff dentro de cada batch\n",
    "    - Calcula una distribución nula de coeficientes para cada genotipo\n",
    "    - Retorna los valores p empíricos\n",
    "    \"\"\"\n",
    "    observed_results = run_gwas(df)\n",
    "    permuted_coefs = {genotype: [] for genotype in observed_results['genotype']}\n",
    "    \n",
    "    for _ in range(num_permutations):\n",
    "        permuted_df = df.copy()\n",
    "        \n",
    "        for batch in permuted_df['batch_id'].unique():\n",
    "            subset = permuted_df[permuted_df['batch_id'] == batch].copy()\n",
    "            permuted_df.loc[subset.index, 'phenotype_diff'] = np.random.permutation(subset['phenotype_diff'].values)\n",
    "        \n",
    "        permuted_results = run_gwas(permuted_df)\n",
    "        for genotype, coef in zip(permuted_results['genotype'], permuted_results['coef']):\n",
    "            permuted_coefs[genotype].append(coef)\n",
    "    \n",
    "    # Calcular valores p empíricos\n",
    "    empirical_p_values = []\n",
    "    for _, row in observed_results.iterrows():\n",
    "        genotype = row['genotype']\n",
    "        empirical_p = (100 - percentileofscore(permuted_coefs[genotype], row['coef'])) / 100\n",
    "        empirical_p_values.append(empirical_p)\n",
    "    \n",
    "    observed_results['empirical_p'] = empirical_p_values\n",
    "    return observed_results\n",
    "\n",
    "# Ejemplo de uso\n",
    "# df = pd.read_csv(\"datos_gwas.csv\")  # Cargar los datos reales\n",
    "# results = permutation_test(df, num_permutations=1000)\n",
    "# results.to_csv(\"gwas_permutation_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def select_association(index=widgets.IntSlider(min=0,max=100)):\n",
    "    \n",
    "    SNP       = gwas_results.SNP.iloc[index]\n",
    "    phenotype_name = gwas_results.phenotype.iloc[index]\n",
    "    p_value   = gwas_results.p_value.iloc[index]\n",
    "    \n",
    "    genotype = all_data[SNP]\n",
    "    phenotype = all_data[phenotype_name]\n",
    "    n_points = genotype.isna().__invert__().sum()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.scatter(genotype, phenotype);\n",
    "    plt.title(f\"{SNP}\\n{phenotype_name} ({p_value:.1e}, {n_points})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gff = \"/home/rodrigo/01_repos/plant-microbiota-interaction/data/genomes/reference_BM_TG1E1/annotations/annotations2.gff3\"\n",
    "gff = \"/home/rodrigo/01_repos/plant-microbiota-interaction/data/genomes/reference_BM_TG1E1/annotations/annotations.gff3\"\n",
    "# gff = \"/home/rodrigo/01_repos/plant-microbiota-interaction/data/genomes/reference/annotations_translated_from_tg1e1.gff\"\n",
    "gff_data = read_annotation_data(gff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwas_results.SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_results[\"annotation\"] = annotate_results(gwas_results, gff_data.query(\"type == 'CDS'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(signed_dist1, signed_dist2):\n",
    "\n",
    "    if signed_dist1 * signed_dist2 < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return min(abs(signed_dist1), abs(signed_dist2))\n",
    "\n",
    "\n",
    "def get_closest_gene(df):\n",
    "\n",
    "    dist_to_start = df.start - df.position\n",
    "    dist_to_end   = df.end   - df.position\n",
    "    pp = pd.DataFrame([dist_to_start, dist_to_end]).T\n",
    "    \n",
    "    min_index = pp.apply(lambda row: compute_distance(row[0], row[1]), axis=1).argmin()\n",
    "    return df.iloc[min_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_gwas_same_contig = pd.merge(gwas_results, gff_data.query(\"type == 'CDS'\"), left_on=\"contig\", right_on=\"seqid\")\n",
    "mapped_genes = annotated_gwas_same_contig.groupby(\"SNP\").apply(get_closest_gene, include_groups=False)# .attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_results.drop([\"annotation\", \"SNP\", \"beta\", \"r_squared\"], axis=1)[[\"contig\", \"position\", \"phenotype\", \"p_value\"]].to_csv(\"gwas_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_genes.reset_index().apply(lambda x: [x.SNP, x.start, x.end, x.strand, x.attributes['product']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pd.DataFrame(mapped_genes.reset_index().apply(lambda x: [x.SNP[0], x.SNP[1], x.start, x.end, x.strand, x.attributes['product']], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk.to_csv(\"gene_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_genes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_genes.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manhattan_static(gwas_results)\n",
    "manhattan_interactive(gwas_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.phenotype.unique()\n",
    "# results_per_pheno_df = results_df.query(\"phenotype == 'LR_Density'\")\n",
    "# results_per_pheno_df = results_per_pheno_df.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def get_contig_range(input_fasta, contig_name, start=None, end=None):\n",
    "    \"\"\"\n",
    "    Retrieves a specific contig (and optionally a range within it) as a string.\n",
    "\n",
    "    Parameters:\n",
    "    - input_fasta: str, path to the input FASTA file.\n",
    "    - contig_name: str, the name of the contig to query.\n",
    "    - start: int or None, start position (1-based, inclusive). Default is None for full contig.\n",
    "    - end: int or None, end position (1-based, inclusive). Default is None for full contig.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted sequence as a string.\n",
    "    - None: If the contig is not found.\n",
    "    \"\"\"\n",
    "    for record in SeqIO.parse(input_fasta, \"fasta\"):\n",
    "        if record.id == contig_name:\n",
    "            if start is not None and end is not None:\n",
    "                return str(record.seq[start - 1:end])\n",
    "            return str(record.seq)\n",
    "    print(f\"Contig '{contig_name}' not found in {input_fasta}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_file = \"data/genomes/reference_2/full_sequence.fasta\"\n",
    "contig = \"PRKV01000004.1\"\n",
    "start_position = 394240 - 200\n",
    "end_position = 394240 + 200\n",
    "\n",
    "get_contig_range(input_file, contig, start_position, end_position)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scRNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
