{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from ipywidgets import (\n",
    "    interact,\n",
    "    IntSlider,\n",
    "    RadioButtons\n",
    ")\n",
    "\n",
    "from genome_helpers import (\n",
    "    get_genome_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_bases(row):\n",
    "    \n",
    "    base_calls = row[\"bases\"]\n",
    "    quality_scores = phred_quality(row[\"qual\"])\n",
    "    ref_base = row[\"ref\"].upper()\n",
    "\n",
    "    base_calls = base_calls.replace(\"$\", \"\").replace(\"^]\", \"\").replace(\"^I\", \"\")\n",
    "    \n",
    "    processed_bases = []\n",
    "    passing_quality_scores = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < row[\"depth\"]:\n",
    "        char = base_calls[i]\n",
    "\n",
    "        # Filtrar por calidad (descartar bases con calidad < 20)\n",
    "        if quality_scores[i] < 20:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        passing_quality_scores.append(quality_scores[i])\n",
    "        \n",
    "        # Reemplazar referencia\n",
    "        if char in \".,\":  \n",
    "            processed_bases.append(ref_base)\n",
    "\n",
    "        # Contar bases normales\n",
    "        elif char.upper() in \"ACTG\":\n",
    "            processed_bases.append(char.upper())\n",
    "\n",
    "        # Contar deleciones en la referencia (`*`)\n",
    "        elif char == \"*\":\n",
    "            processed_bases.append(\"D\")\n",
    "\n",
    "        # Detectar inserciones (`+nX`)\n",
    "        elif char == \"+\":\n",
    "            match = re.match(r\"\\+(\\d+)\", base_calls[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                inserted_seq = base_calls[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "                # processed_bases.append(f\"INS_{inserted_seq.upper()}\")\n",
    "                processed_bases.append(\"I\")\n",
    "                i += len(match.group(1)) + num_bases\n",
    "\n",
    "        # Detectar deleciones (`-nX`)\n",
    "        elif char == \"-\":\n",
    "            match = re.match(r\"\\-(\\d+)\", base_calls[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                deleted_seq = base_calls[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "                processed_bases.append(\"D\")\n",
    "                # processed_bases.append(f\"DEL_{deleted_seq.upper()}\")\n",
    "                i += len(match.group(1)) + num_bases\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if len(passing_quality_scores) == 0:\n",
    "         allele_counts = { 'A': 0, 'C': 0, 'T': 0, 'G': 0, 'DEL': 0, 'INS': 0, \"Avg_Qual\": None, \"depth\": 0, \"depth_high_q\": 0 }\n",
    "         return allele_counts\n",
    "\n",
    "    allele_counts = {\n",
    "        'A': processed_bases.count('A'),\n",
    "        'C': processed_bases.count('C'),\n",
    "        'T': processed_bases.count('T'),\n",
    "        'G': processed_bases.count('G'),\n",
    "        'DEL': processed_bases.count('D'),\n",
    "        'INS': processed_bases.count('I')\n",
    "    }\n",
    "\n",
    "    # Contar inserciones y deleciones específicas\n",
    "    for item in processed_bases:\n",
    "        if item.startswith(\"INS_\"):\n",
    "            allele_counts[item] = allele_counts.get(item, 0) + 1\n",
    "        if item.startswith(\"DEL_\"):\n",
    "            allele_counts[item] = allele_counts.get(item, 0) + 1\n",
    "\n",
    "    # Calcular calidad promedio\n",
    "    # allele_counts[\"Avg_Qual\"] = np.mean(quality_scores) if quality_scores else 0\n",
    "    allele_counts[\"Avg_Qual\"] = sum(passing_quality_scores) / len(passing_quality_scores) # if quality_scores else 0\n",
    "    allele_counts[\"depth\"] = row[\"depth\"]\n",
    "    allele_counts[\"depth_high_q\"] = len(passing_quality_scores)\n",
    "    allele_counts[\"bases\"] = processed_bases\n",
    "    allele_counts[\"original_bases\"] = row[\"bases\"]\n",
    "    allele_counts[\"ref_base\"] = ref_base\n",
    "    allele_counts[\"quality_scores\"] = quality_scores\n",
    "    \n",
    "    return allele_counts\n",
    "\n",
    "\n",
    "def mismatch(row):\n",
    "    alleles = [ row[x] for x in ['A', 'C', 'T', 'G', 'DEL', 'INS'] ] #, row.C, row.T, row.G]#, row.DEL, row.INS]\n",
    "    # print(sum(alleles) != row.depth_high_q)\n",
    "    return sum(alleles) != row.depth_high_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phred_quality(qual_string):\n",
    "    return [ord(q) - 33 for q in qual_string]\n",
    "\n",
    "\n",
    "def count_alleles(row):\n",
    "\n",
    "    base_calls = row[\"bases\"]    \n",
    "    print(f\"{len(base_calls)=}\")\n",
    "\n",
    "    quality_scores = phred_quality(row[\"qual\"])\n",
    "    print(f\"{quality_scores=}\")\n",
    "    print(f\"{len(quality_scores)=}\")\n",
    "    \n",
    "    ref_base = row[\"ref\"].upper()\n",
    "    print(f\"{ref_base=}\")\n",
    "\n",
    "    base_calls = re.sub(pattern=\"\\^.\", repl=\"\", string=base_calls)\n",
    "    \n",
    "    # base_calls.replace('$', \"\").replace('^]', \"\").replace('^I', \"\")\n",
    "\n",
    "    n_bases = len(base_calls.replace(\"-\", \"\").replace(\"+\", \"\"))\n",
    "\n",
    "    # Reemplazar '.' y ',' con la base de referencia\n",
    "    base_calls = base_calls.replace('.', ref_base).replace(',', ref_base)\n",
    "    print(f\"{base_calls=}\")\n",
    "    print(f\"{len(base_calls)=}\")\n",
    "    \n",
    "    filtered_bases = \"\".join([base_calls[i] for i in range(n_bases) if quality_scores[i] >= 20])\n",
    "    print(filtered_bases)\n",
    "    filtered_quality_scores = sum([quality_scores[i] for i in range(n_bases) if quality_scores[i] >= 20]) /n_bases\n",
    "    \n",
    "    counts = {\n",
    "        'A': filtered_bases.count('A'),\n",
    "        'C': filtered_bases.count('C'),\n",
    "        'T': filtered_bases.count('T'),\n",
    "        'G': filtered_bases.count('G'),\n",
    "        'DEL': filtered_bases.count('*')  # Conteo de deleciones\n",
    "    }\n",
    "\n",
    "    # Buscar inserciones y deleciones en la secuencia de bases\n",
    "    insertions = re.findall(r'\\+(\\d+)([ACGTNacgtn]+)', filtered_bases)\n",
    "    deletions = re.findall(r'\\-(\\d+)([ACGTNacgtn]+)', filtered_bases)\n",
    "\n",
    "    for size, seq in insertions:\n",
    "        key = f\"INS_{ref_base}{seq.upper()}\"\n",
    "        counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "    for size, seq in deletions:\n",
    "        key = f\"DEL_{seq.upper()}\"\n",
    "        counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "    \n",
    "    counts[\"Avg_Qual\"] = filtered_quality_scores # np.mean(quality_scores) if quality_scores else 0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "\n",
    "def parse_bases(base_string, ref_base):\n",
    "\n",
    "    i = 0\n",
    "    bases_list = []\n",
    "    \n",
    "    while i < len(base_string):\n",
    "        \n",
    "        char = base_string[i]\n",
    "\n",
    "        # Reemplazar referencia\n",
    "        if char in \".,\":  \n",
    "            current_base = ref_base\n",
    "            bases_list.append(current_base)\n",
    "\n",
    "        # Contar bases normales\n",
    "        elif char.upper() in \"ACTG\":\n",
    "            current_base = char.upper()\n",
    "            bases_list.append(current_base)\n",
    "\n",
    "        # Contar deleciones en la referencia (`*`)\n",
    "        elif char == \"*\":\n",
    "            bases_list.append(\"DEL\")\n",
    "\n",
    "        # Detectar inserciones (`+nX`) y asociarlas correctamente\n",
    "        elif char == \"+\":\n",
    "            match = re.match(r\"\\+(\\d+)\", base_string[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                inserted_seq = base_string[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "\n",
    "                # Asociar la inserción a la base previa (última base agregada)\n",
    "                if bases_list:\n",
    "                    bases_list[-1] = f\"{bases_list[-1]}_INS_{inserted_seq.upper()}\"\n",
    "                else:\n",
    "                    bases_list.append(f\"INS_{inserted_seq.upper()}\")  # Caso extremo: inserción sin base previa\n",
    "\n",
    "                i += len(match.group(1)) + num_bases  # Salteamos la inserción en el string\n",
    "\n",
    "        # Detectar deleciones (`-nX`)\n",
    "        elif char == \"-\":\n",
    "            match = re.match(r\"\\-(\\d+)\", base_string[i:])\n",
    "            if match:\n",
    "                num_bases = int(match.group(1))\n",
    "                deleted_seq = base_string[i+len(match.group(1))+1:i+len(match.group(1))+1+num_bases]\n",
    "\n",
    "                # Asociar la deleción a la base previa (última base agregada)\n",
    "                if bases_list:\n",
    "                    bases_list[-1] = f\"{bases_list[-1]}_DEL_{deleted_seq.upper()}\"\n",
    "                else:\n",
    "                    bases_list.append(f\"DEL_{deleted_seq.upper()}\")\n",
    "\n",
    "                i += len(match.group(1)) + num_bases  # Salteamos la deleción en el string\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return bases_list\n",
    "\n",
    "\n",
    "def process_bases(row):\n",
    "    \n",
    "    base_calls = row[\"bases\"]\n",
    "    quality_scores = phred_quality(row[\"qual\"])\n",
    "    ref_base = row[\"ref\"].upper()\n",
    "\n",
    "    base_calls = base_calls.replace(\"$\", \"\").replace(\"^]\", \"\").replace(\"^I\", \"\")\n",
    "\n",
    "    parsed_bases = parse_bases(base_calls, ref_base)\n",
    "\n",
    "    return [Counter(parsed_bases), quality_scores]\n",
    "\n",
    "\n",
    "def process_folder(folder=\"data/genomes/alignments_paired_end\"):\n",
    "    \n",
    "    for raiz, carpetas, archivos in os.walk(folder):\n",
    "        for archivo in archivos:        \n",
    "    \n",
    "            if not (\"counts\" in archivo and archivo.endswith(\"txt\")):\n",
    "                continue\n",
    "    \n",
    "            ruta_completa = os.path.join(raiz, archivo)\n",
    "            batch = os.path.basename(ruta_completa).split(\"__\")[0]        \n",
    "    \n",
    "            df = pd.read_csv(ruta_completa, sep=\"\\t\", header=None)\n",
    "            df.columns = [\"contig\", \"position\", \"ref\", \"depth\", \"bases\", \"qual\"]\n",
    "            df = pd.concat([df, df.apply(lambda x: process_bases(x)[0], axis=1), df.apply(lambda x: process_bases(x)[1], axis=1)], axis=1)\n",
    "            df.columns = df.columns[:-2].to_list() + [\"count\", \"quality\"]\n",
    "            df = df.drop([\"qual\"], axis=1)\n",
    "            df = df.assign(sample=batch)\n",
    "            \n",
    "            (pp := locals().get(\"pp\") or []).append(df)\n",
    "    \n",
    "    pp = pd.concat(pp)[[\"contig\", \"position\", \"sample\", \"count\"]].pivot(index=[\"contig\", \"position\"], columns=\"sample\", values=\"count\")\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_from_counts(allele_dict):\n",
    "    \n",
    "    assert isinstance(allele_dict, dict) or np.isnan(allele_dict)\n",
    "\n",
    "    if not isinstance(allele_dict, dict) and np.isnan(allele_dict):\n",
    "        return dict(depth=0, A=0, T=0, G=0, C=0)\n",
    "    else:\n",
    "        depth = sum(allele_dict.values())\n",
    "        return dict(\n",
    "            depth=depth, \n",
    "            A=allele_dict.get(\"A\", 0), \n",
    "            T=allele_dict.get(\"T\", 0), \n",
    "            G=allele_dict.get(\"G\", 0), \n",
    "            C=allele_dict.get(\"C\", 0))\n",
    "    \n",
    "\n",
    "def plot_allele_freqs(snp_evolution_df):\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    for allele in [\"A\", \"C\", \"T\", \"G\"]:\n",
    "        plt.plot(snp_evolution_df.index, snp_evolution_df[allele], label=f'Alelo {allele}')\n",
    "\n",
    "    if \"depth\" in snp_evolution_df.columns:\n",
    "        plt.plot(snp_evolution_df.index, snp_evolution_df.depth, label=f'Cobertura')\n",
    "\n",
    "    plt.xlabel('Generaciones')\n",
    "    plt.ylabel('Frecuencia alélica')\n",
    "    plt.title('Distribución de alelos a lo largo de generaciones')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = ( (df := process_folder())\n",
    "    .map(freq_from_counts)\n",
    "    .melt(ignore_index=False) )\n",
    "\n",
    "batch_mapping = get_genome_metadata(as_dataframe=False)\n",
    "samples = freq_df['sample'].apply(lambda x: batch_mapping.get(x, (x, -1, -1)))\n",
    "samples_df = pd.DataFrame(samples.to_list(), columns=[\"treatment\", \"replica\", \"generation\"])\n",
    "\n",
    "freq_df = pd.concat([samples_df, freq_df.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_DEPTH = 50\n",
    "\n",
    "@interact\n",
    "def interactive_allele_evolution_plot(i=IntSlider(min=0, max=100), freq_or_count=RadioButtons(options=[\"cuentas\", \"frecuencia\"])):\n",
    "\n",
    "    SNPs = freq_df.drop_duplicates(subset=[\"contig\", \"position\"])[[\"contig\", \"position\"]]\n",
    "    contig, position = SNPs.contig.iloc[i], SNPs.position.iloc[i]\n",
    "\n",
    "    TREATMENT, REPLICA = \"MS\", 3\n",
    "\n",
    "    snp_evolution = ( freq_df\n",
    "        .query(\"treatment == @TREATMENT and replica == @REPLICA\")\n",
    "        .query(\"contig == @contig and position == @position\")\n",
    "        .sort_values(\"generation\") )\n",
    "    \n",
    "    snp_count_evol = snp_evolution.value.apply(lambda x: [x[\"A\"], x[\"C\"], x[\"T\"], x[\"G\"], x['depth']]).set_axis(snp_evolution.generation)\n",
    "    snp_count_evol = pd.DataFrame(snp_count_evol.apply(lambda x: list(x)).to_list(), columns=[\"A\", \"C\", \"T\", \"G\", \"depth\"])    \n",
    "    snp_count_evol = snp_count_evol.query(\"depth > @MINIMUM_DEPTH\")\n",
    "\n",
    "    snp_freq_evol = ( snp_count_evol\n",
    "        .assign(freqA=snp_count_evol.A/snp_count_evol.depth, freqC=snp_count_evol.C/snp_count_evol.depth, freqT=snp_count_evol['T']/snp_count_evol.depth, freqG=snp_count_evol.G/snp_count_evol.depth)\n",
    "        .drop([\"A\", \"C\", \"T\", \"G\", \"depth\"], axis=1)\n",
    "        .rename({ f\"freq{a}\": a for a in [\"A\", \"C\", \"T\", \"G\"] }, axis=1))\n",
    "    \n",
    "    plot_allele_freqs(snp_freq_evol if freq_or_count == \"frecuencia\" else snp_count_evol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(count_dict):\n",
    "    \n",
    "    assert 'depth' in count_dict\n",
    "    \n",
    "    count_dict = count_dict.copy() \n",
    "    depth = count_dict.pop('depth')\n",
    "    major_allele_count = max(count_dict.values())\n",
    "    if depth < 20:\n",
    "        return np.nan\n",
    "    \n",
    "    return major_allele_count / depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_contig_and_position(df): \n",
    "    return df.assign(variant_id=df[[\"contig\", \"position\"]].apply(tuple, axis=1))\n",
    "\n",
    "major_allele_freq_df = ( \n",
    "    freq_df.assign(value=freq_df.value.apply(get_freq))\n",
    "    .query(\"value.notna()\")\n",
    "    .assign(replica=lambda x: x['replica'].astype(int))\n",
    "    .drop(\"sample\", axis=1)\n",
    "    .pivot(\n",
    "        columns=[\"treatment\", \"replica\", \"generation\"], \n",
    "        index=[\"contig\", \"position\"], values=\"value\")\n",
    "    .reset_index()\n",
    "    .pipe(merge_contig_and_position)\n",
    "    .set_index('variant_id')\n",
    "    .drop([\"contig\", \"position\"], axis=1) )\n",
    "\n",
    "major_allele_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQ_WIDE_PKL = \"freq_dataframe_wide_ref1.pkl\"\n",
    "# assert not os.path.exists(FREQ_WIDE_PKL), f\"{FREQ_WIDE_PKL} already exists, not overwriting it.\"\n",
    "\n",
    "pkl.dump(major_allele_freq_df, open(FREQ_WIDE_PKL, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_gwas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
